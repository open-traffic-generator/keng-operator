============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
metadata: {'Python': '3.10.12', 'Platform': 'Linux-6.2.0-39-generic-x86_64-with-glibc2.35', 'Packages': {'pytest': '8.0.0', 'pluggy': '1.4.0'}, 'Plugins': {'html': '3.2.0', 'metadata': '3.0.0'}}
rootdir: /home/ixia_c_cicd/keng-operator/tests/py
configfile: pytest.ini
plugins: html-3.2.0, metadata-3.0.0
collecting ... collected 1 item

py/default/test_liveness_custom_config.py::test_liveness_custom_config Executing `kubectl get pods -n ixiatg-op-system -o 'jsonpath={.items[0].status.containerStatuses[?(@.name=="manager")].restartCount}'` ...
Output:
0
Error:

Operator Pod Restart Count: 0
[Namespace:ixia-c]Deploying KNE topology
Loading custom liveness config...
Executing `sudo -u root cat ./deployments/ixia-c-config.yaml` ...
Output:
apiVersion: v1
kind: ConfigMap
metadata:
    name: ixiatg-release-config
    namespace: ixiatg-op-system
data:
    versions: |
        {
          "release": "local",
          "images": [
                {
                    "name": "controller",
                    "path": "ghcr.io/open-traffic-generator/keng-controller",
                    "tag": "0.1.0-222",
                    "env": {
                        "TRACE": "true"
                    }
                },
                {
                    "name": "gnmi-server",
                    "path": "ghcr.io/open-traffic-generator/otg-gnmi-server",
                    "tag": "1.13.7"
                },
                {
                    "name": "traffic-engine",
                    "path": "ghcr.io/open-traffic-generator/ixia-c-traffic-engine",
                    "tag": "1.6.0.109"
                },
                {
                    "name": "protocol-engine",
                    "path": "ghcr.io/open-traffic-generator/ixia-c-protocol-engine",
                    "tag": "1.00.0.355"
                },
                {
                    "name": "ixhw-server",
                    "path": "ghcr.io/open-traffic-generator/keng-layer23-hw-server",
                    "tag": "0.13.7-1"
                },
                {
                    "name": "license-server",
                    "path": "ghcr.io/open-traffic-generator/licensed/keng-license-server",
                    "tag": "latest"
                }
            ]
        }

Error:

{'apiVersion': 'v1', 'kind': 'ConfigMap', 'metadata': {'name': 'ixiatg-release-config', 'namespace': 'ixiatg-op-system'}, 'data': {'versions': '{"release": "local", "images": [{"name": "controller", "path": "ghcr.io/open-traffic-generator/keng-controller", "tag": "0.1.0-222", "env": {"TRACE": "true"}, "liveness-failure": 3}, {"name": "gnmi-server", "path": "ghcr.io/open-traffic-generator/otg-gnmi-server", "tag": "1.13.7", "liveness-enable": false}, {"name": "traffic-engine", "path": "ghcr.io/open-traffic-generator/ixia-c-traffic-engine", "tag": "1.6.0.109", "liveness-period": 5}, {"name": "protocol-engine", "path": "ghcr.io/open-traffic-generator/ixia-c-protocol-engine", "tag": "1.00.0.355", "liveness-initial-delay": 12}, {"name": "ixhw-server", "path": "ghcr.io/open-traffic-generator/keng-layer23-hw-server", "tag": "0.13.7-1"}, {"name": "license-server", "path": "ghcr.io/open-traffic-generator/licensed/keng-license-server", "tag": "latest"}]}'}}
Executing `kubectl apply -f custom-ixia-c-config.yaml` ...
Output:
configmap/ixiatg-release-config unchanged

Error:

configmap custom-ixia-c-config.yaml applied inside kind container


Waiting for ensured topology does not exists ...
Executing `kubectl get topology -n ixia-c` ...
Output:

Error:
No resources found in ixia-c namespace.

Done waiting for ensured topology does not exists
Executing `kne create ./topology/ixia_c_pd_rest_topology.yaml` ...
Output:

Error:
I0130 12:23:30.414861   16655 root.go:119] /home/ixia_c_cicd/keng-operator/tests/topology
I0130 12:23:30.418054   16655 topo.go:118] Trying in-cluster configuration
I0130 12:23:30.418103   16655 topo.go:121] Falling back to kubeconfig: "/home/ixia_c_cicd/.kube/config"
I0130 12:23:30.421965   16655 topo.go:254] Adding Link: otg:eth1 arista1:eth1
I0130 12:23:30.422008   16655 topo.go:292] Adding Node: otg:KEYSIGHT
I0130 12:23:30.422033   16655 topo.go:292] Adding Node: arista1:ARISTA
I0130 12:23:30.451436   16655 topo.go:359] Creating namespace for topology: "ixia-c"
I0130 12:23:30.470709   16655 topo.go:369] Server Namespace: &Namespace{ObjectMeta:{ixia-c    89f67b41-32ce-4cc8-9906-ce0a7c54232d 252116 0 2024-01-30 12:23:30 +0530 IST <nil> <nil> map[kubernetes.io/metadata.name:ixia-c] map[] [] [] [{kne Update v1 2024-01-30 12:23:30 +0530 IST FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:kubernetes.io/metadata.name":{}}}} }]},Spec:NamespaceSpec{Finalizers:[kubernetes],},Status:NamespaceStatus{Phase:Active,Conditions:[]NamespaceCondition{},},}
I0130 12:23:30.471522   16655 topo.go:396] Getting topology specs for namespace ixia-c
I0130 12:23:30.471543   16655 topo.go:325] Getting topology specs for node arista1
I0130 12:23:30.471562   16655 topo.go:325] Getting topology specs for node otg
I0130 12:23:30.471598   16655 ixia.go:131] Getting interfaces for ixia node resource otg ...
I0130 12:23:30.471611   16655 ixia.go:41] Creating new ixia CRD for node: otg
I0130 12:23:30.471628   16655 ixia.go:135] Creating custom resource for ixia (desiredState=INITIATED) ...
I0130 12:23:30.492984   16655 ixia.go:105] Waiting for ixia CRD state to be INITIATED ... (timeout: 30s)
I0130 12:23:33.175618   16655 ixia.go:118] Attained ixia CRD state INITIATED
I0130 12:23:33.175707   16655 topo.go:403] Creating topology for meshnet node arista1
I0130 12:23:33.194677   16655 topo.go:403] Creating topology for meshnet node otg-port-eth1
I0130 12:23:33.210075   16655 topo.go:376] Creating Node Pods
I0130 12:23:33.210132   16655 ixia.go:194] Creating deployment for node resource otg
I0130 12:23:33.217816   16655 ixia.go:210] Updating ixia CRD (desiredState=DEPLOYED) ...
I0130 12:23:33.244744   16655 topo.go:381] Node "otg" resource created
I0130 12:23:33.264793   16655 ceos.go:102] Creating new CEosLabDevice CRD for node: arista1
I0130 12:23:43.954186   16655 ceos.go:195] Created CEosLabDevice CRD for node: arista1
I0130 12:23:43.954259   16655 topo.go:381] Node "arista1" resource created
I0130 12:23:55.376318   16655 topo.go:449] Node "arista1": Status RUNNING
I0130 12:23:57.569866   16655 topo.go:449] Node "otg": Status RUNNING
I0130 12:23:57.670957   16655 topo.go:159] Topology "ixia-c" created
Log files can be found in:
    /tmp/kne.CI-runner.ixia_c_cicd.log.INFO.20240130-122330.16655

[Namespace:ixia-c]Verifying pods count in KNE topology


Waiting for pods count to be as expected ...
Executing `kubectl get pods -n ixia-c | grep -v RESTARTS | wc -l` ...
Output:
3

Error:

Actual pods: 3 - Expected: 3
Done waiting for pods count to be as expected
[Namespace:ixia-c]Verifying pods status in KNE topology


Waiting for pods status to be as expected ...
Executing `kubectl get pods -n ixia-c | grep Running | wc -l` ...
Output:
3

Error:

Actual Running pods: 3 - Expected: 3
Done waiting for pods status to be as expected
[Namespace:ixia-c]Verifying individual pods in KNE topology
Executing `kubectl describe pods/otg-controller -n ixia-c` ...
Output:
Name:             otg-controller
Namespace:        ixia-c
Priority:         0
Service Account:  default
Node:             kind-control-plane/172.18.0.2
Start Time:       Tue, 30 Jan 2024 12:23:35 +0530
Labels:           app=otg-controller
Annotations:      <none>
Status:           Running
IP:               10.244.0.104
IPs:
  IP:  10.244.0.104
Containers:
  gnmi:
    Container ID:  containerd://afa03814fb257b4125fcfe34438b0f777c4189d10d1522ea82151e39a9d1b40c
    Image:         ghcr.io/open-traffic-generator/otg-gnmi-server:1.13.7
    Image ID:      docker.io/library/import-2024-01-29@sha256:8c5f2da770b1b3c6b0e1cf6fede3bd8b41ab981033375ec76d5102f4e6198580
    Port:          50051/TCP
    Host Port:     0/TCP
    Args:
      -http-server
      https://localhost:8443
      --debug
    State:          Running
      Started:      Tue, 30 Jan 2024 12:23:37 +0530
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     15Mi
    Liveness:     tcp-socket :50051 delay=1s timeout=1s period=10s #success=1 #failure=6
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xfs48 (ro)
  ixia-c:
    Container ID:  containerd://b7c3f0e0f32ddfa79c6f577e066bcfb85cfbdfdc1371929a53e21ba72daf49c7
    Image:         ghcr.io/open-traffic-generator/keng-controller:0.1.0-222
    Image ID:      docker.io/library/import-2024-01-29@sha256:3bc63ddb30d9f28fd4e95e62610c02eb1a7a96d6a5dc01e71bc34ce6b5a85a1e
    Port:          <none>
    Host Port:     <none>
    Args:
      --accept-eula
      --debug
    State:          Running
      Started:      Tue, 30 Jan 2024 12:23:37 +0530
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     25Mi
    Liveness:     tcp-socket :40051 delay=1s timeout=1s period=10s #success=1 #failure=6
    Environment:  <none>
    Mounts:
      /home/ixia-c/controller/config from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xfs48 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      controller-config
    Optional:  false
  kube-api-access-xfs48:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  23s   default-scheduler  Successfully assigned ixia-c/otg-controller to kind-control-plane
  Normal  Pulled     22s   kubelet            Container image "ghcr.io/open-traffic-generator/otg-gnmi-server:1.13.7" already present on machine
  Normal  Created    22s   kubelet            Created container gnmi
  Normal  Started    21s   kubelet            Started container gnmi
  Normal  Pulled     21s   kubelet            Container image "ghcr.io/open-traffic-generator/keng-controller:0.1.0-222" already present on machine
  Normal  Created    21s   kubelet            Created container ixia-c
  Normal  Started    21s   kubelet            Started container ixia-c

Error:

Executing `kubectl describe pods/otg-port-eth1 -n ixia-c` ...
Output:
Name:             otg-port-eth1
Namespace:        ixia-c
Priority:         0
Service Account:  default
Node:             kind-control-plane/172.18.0.2
Start Time:       Tue, 30 Jan 2024 12:23:36 +0530
Labels:           app=otg-port-eth1
                  topo=ixia-c
Annotations:      <none>
Status:           Running
IP:               10.244.0.105
IPs:
  IP:  10.244.0.105
Init Containers:
  init-container:
    Container ID:  containerd://8e352b2515e13c7198f52d83d2777db80d3fb631cc6faf620720b4de11acf3b4
    Image:         networkop/init-wait:latest
    Image ID:      docker.io/networkop/init-wait@sha256:a54e253ce78be8ea66942051296c3676e254bc37460e19a9eb540517faaaf4d7
    Port:          <none>
    Host Port:     <none>
    Args:
      2
      10
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 30 Jan 2024 12:23:37 +0530
      Finished:     Tue, 30 Jan 2024 12:23:55 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbdlv (ro)
Containers:
  otg-port-eth1-traffic-engine:
    Container ID:   containerd://8509a4f261ae95a15a76d9cf4eb6a22f97763591547375fa3383f7c69babd41d
    Image:          ghcr.io/open-traffic-generator/ixia-c-traffic-engine:1.6.0.109
    Image ID:       docker.io/library/import-2024-01-29@sha256:51a49a73481e6f46d8fdaf6f796ed11c0bf1152a3686e2aaca36f924441872b9
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 30 Jan 2024 12:23:55 +0530
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  60Mi
    Liveness:  tcp-socket :5555 delay=1s timeout=1s period=10s #success=1 #failure=6
    Environment:
      DEFAULT_PORT_SPEED:  1000
      OPT_LISTEN_PORT:     5555
      ARG_CORE_LIST:       2 3 4
      ARG_IFACE_LIST:      virtual@af_packet,eth1
      OPT_NO_HUGEPAGES:    Yes
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbdlv (ro)
  otg-port-eth1-protocol-engine:
    Container ID:   containerd://4db08e267ae8f8e1c6e915989904b8558a419659a181b15780a51e3e74e462f1
    Image:          ghcr.io/open-traffic-generator/ixia-c-protocol-engine:1.00.0.355
    Image ID:       docker.io/library/import-2024-01-29@sha256:929412338f6a851c173f37a4aae55480a45775e7bce6fa97916138320f343d86
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 30 Jan 2024 12:23:56 +0530
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  350Mi
    Liveness:  tcp-socket :50071 delay=1s timeout=1s period=10s #success=1 #failure=6
    Environment:
      INTF_LIST:  eth1
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sbdlv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-sbdlv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  22s   default-scheduler  Successfully assigned ixia-c/otg-port-eth1 to kind-control-plane
  Normal  Pulled     22s   kubelet            Container image "networkop/init-wait:latest" already present on machine
  Normal  Created    22s   kubelet            Created container init-container
  Normal  Started    21s   kubelet            Started container init-container
  Normal  Pulled     3s    kubelet            Container image "ghcr.io/open-traffic-generator/ixia-c-traffic-engine:1.6.0.109" already present on machine
  Normal  Created    3s    kubelet            Created container otg-port-eth1-traffic-engine
  Normal  Started    3s    kubelet            Started container otg-port-eth1-traffic-engine
  Normal  Pulled     3s    kubelet            Container image "ghcr.io/open-traffic-generator/ixia-c-protocol-engine:1.00.0.355" already present on machine
  Normal  Created    3s    kubelet            Created container otg-port-eth1-protocol-engine
  Normal  Started    2s    kubelet            Started container otg-port-eth1-protocol-engine

Error:

Executing `kubectl describe pods/arista1 -n ixia-c` ...
Output:
Name:             arista1
Namespace:        ixia-c
Priority:         0
Service Account:  default
Node:             kind-control-plane/172.18.0.2
Start Time:       Tue, 30 Jan 2024 12:23:43 +0530
Labels:           app=arista1
                  model=ceos
                  os=eos
                  topo=ixia-c
                  vendor=ARISTA
                  version=
Annotations:      <none>
Status:           Running
IP:               10.244.0.106
IPs:
  IP:           10.244.0.106
Controlled By:  CEosLabDevice/arista1
Init Containers:
  init-arista1:
    Container ID:  containerd://668b7fd5c21aaf9823f90653e00a87143e90d63cd1f8e5f872ae0ee3f2960635
    Image:         networkop/init-wait:latest
    Image ID:      docker.io/networkop/init-wait@sha256:a54e253ce78be8ea66942051296c3676e254bc37460e19a9eb540517faaaf4d7
    Port:          <none>
    Host Port:     <none>
    Args:
      2
      0
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 30 Jan 2024 12:23:45 +0530
      Finished:     Tue, 30 Jan 2024 12:23:45 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89xtz (ro)
Containers:
  ceos:
    Container ID:  containerd://05e887fefaae62b806d217defd83f494520eb72b1eaa4cb1f4fff430d7306659
    Image:         ghcr.io/open-traffic-generator/ceos:4.29.1F-29233963
    Image ID:      docker.io/library/import-2024-01-29@sha256:77607d77934522e4fa90a3b867c97904eee7587fc9c7ae34b4dc836a4d77db64
    Port:          <none>
    Host Port:     <none>
    Command:
      /sbin/init
    Args:
      systemd.setenv=CEOS=1
      systemd.setenv=EOS_PLATFORM=ceoslab
      systemd.setenv=ETBA=1
      systemd.setenv=INTFTYPE=eth
      systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1
      systemd.setenv=container=docker
    State:          Running
      Started:      Tue, 30 Jan 2024 12:23:45 +0530
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     500m
      memory:  1Gi
    Startup:   exec [wfw -t 5] delay=0s timeout=5s period=5s #success=1 #failure=24
    Environment:
      CEOS:                                 1
      EOS_PLATFORM:                         ceoslab
      ETBA:                                 1
      INTFTYPE:                             eth
      SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:  1
      container:                            docker
    Mounts:
      /mnt/flash/EosIntfMapping.json from volume-configmap-intfmapping-arista1 (rw,path="EosIntfMapping.json")
      /mnt/flash/gnmiCert.pem from volume-secret-selfsigned-arista1-0 (rw,path="gnmiCert.pem")
      /mnt/flash/gnmiCertKey.pem from volume-secret-selfsigned-arista1-0 (rw,path="gnmiCertKey.pem")
      /mnt/flash/rc.eos from volume-configmap-rceos-arista1 (rw,path="rc.eos")
      /mnt/flash/startup-config from volume-arista1-config (rw,path="startup-config")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89xtz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  volume-arista1-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      arista1-config
    Optional:  false
  volume-configmap-intfmapping-arista1:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      configmap-intfmapping-arista1
    Optional:  false
  volume-configmap-rceos-arista1:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      configmap-rceos-arista1
    Optional:  false
  volume-secret-selfsigned-arista1-0:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  secret-selfsigned-arista1-0
    Optional:    false
  kube-api-access-89xtz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  15s   default-scheduler  Successfully assigned ixia-c/arista1 to kind-control-plane
  Normal   Pulled     14s   kubelet            Container image "networkop/init-wait:latest" already present on machine
  Normal   Created    14s   kubelet            Created container init-arista1
  Normal   Started    13s   kubelet            Started container init-arista1
  Normal   Pulled     13s   kubelet            Container image "ghcr.io/open-traffic-generator/ceos:4.29.1F-29233963" already present on machine
  Normal   Created    13s   kubelet            Created container ceos
  Normal   Started    13s   kubelet            Started container ceos
  Warning  Unhealthy  4s    kubelet            Startup probe failed: command "wfw -t 5" timed out

Error:

Executing `kubectl get pod/otg-controller -n ixia-c -o 'jsonpath={.spec.containers[?(@.name=="ixia-c")].livenessProbe}'` ...
Output:
{"failureThreshold":6,"initialDelaySeconds":1,"periodSeconds":10,"successThreshold":1,"tcpSocket":{"port":40051},"terminationGracePeriodSeconds":1,"timeoutSeconds":1}
Error:

Reset Configmap...
Executing `kubectl apply -f deployments/ixia-c-config.yaml` ...
Output:
configmap/ixiatg-release-config configured

Error:

configmap deployments/ixia-c-config.yaml applied inside kind container
FAILED

=================================== FAILURES ===================================
_________________________ test_liveness_custom_config __________________________

    def test_liveness_custom_config():
        """
        Deploy b2b kne topology with default version,
        - namespace - 1: ixia-c
        Delete b2b kne topology,
        - namespace - 1: ixia-c
        Validate,
        - custom liveness parameters for protocol engines
        """
        namespace1 = 'ixia-c'
        namespace1_config = 'ixia_c_pd_rest_topology.yaml'
        expected_pods = [
            'otg-controller',
            'otg-port-eth1',
            'arista1'
        ]
        container_extensions = [
            '-protocol-engine',
            '-traffic-engine'
        ]
        probe_params = {'protocol-engine':{'liveness-initial-delay': 12}, 'traffic-engine':{'liveness-period': 5}, 'controller':{'liveness-failure': 3}, 'gnmi-server':{'liveness-enable': False}}
        try:
            op_rscount = utils.get_operator_restart_count()
            print("[Namespace:{}]Deploying KNE topology".format(
                namespace1
            ))
            utils.load_liveness_configmap(probe_params)
            utils.create_kne_config(namespace1_config, namespace1)
            utils.ixia_c_pods_ok(namespace1, expected_pods)
>           utils.check_liveness_data('ixia-c', expected_pods[0], namespace1, True, 1, 10, 3)

py/default/test_liveness_custom_config.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cont = 'ixia-c', pod = 'otg-controller', namespace = 'ixia-c', enabled = True
delay = 1, period = 10, failure = 3

    def check_liveness_data(cont, pod, namespace, enabled=True, delay=0, period=0, failure=0):
        base_cmd = "'jsonpath={.spec.containers[?(@.name==\"" + cont + "\")].livenessProbe"
        base_cmd = "kubectl get pod/{} -n {} -o ".format(pod, namespace) + base_cmd
        cmd = base_cmd + "}'"
        out, _ = exec_shell(cmd, True, True)
        if enabled:
            assert out != "", "Expected liveness to be enabled; no liveness data found"
        else:
            assert out == "", "Expected liveness to be disabled; liveness data non-empty"
            return
        res = json.loads(out)
        if delay != 0:
            key = 'initialDelaySeconds'
            assert key in res.keys() and delay == res[key], "InitialDelaySeconds mismatch, expected {}, found {}".format(delay, res[key])
        if period != 0:
            key = 'periodSeconds'
            assert key in res.keys() and period == res[key], "PeriodSeconds mismatch, expected {}, found {}".format(period, res[key])
        if failure != 0:
            key = 'failureThreshold'
>           assert key in res.keys() and failure == res[key], "FailureThreshold mismatch, expected {}, found {}".format(failure, res[key])
E           AssertionError: FailureThreshold mismatch, expected 3, found 6

py/utils/common.py:648: AssertionError
=========================== short test summary info ============================
FAILED py/default/test_liveness_custom_config.py::test_liveness_custom_config
============================== 1 failed in 30.34s ==============================
